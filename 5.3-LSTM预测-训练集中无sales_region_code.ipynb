{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            sales_region_code  first_cate_code  second_cate_code  item_code  \\\norder_date                                                                    \n2015-09-01                104              307               403      22069   \n2015-09-01                104              301               405      20028   \n2015-09-02                104              307               403      21183   \n2015-09-02                104              308               404      20448   \n2015-09-02                104              307               403      21565   \n...                       ...              ...               ...        ...   \n2018-12-20                102              302               408      20994   \n2018-12-20                102              302               408      21875   \n2018-12-20                102              302               408      20215   \n2018-12-20                102              302               408      20195   \n2018-12-20                102              302               408      20321   \n\n            ord_qty  \norder_date           \n2015-09-01       19  \n2015-09-01       12  \n2015-09-02      109  \n2015-09-02        3  \n2015-09-02        3  \n...             ...  \n2018-12-20       59  \n2018-12-20      502  \n2018-12-20      106  \n2018-12-20      187  \n2018-12-20      205  \n\n[597694 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sales_region_code</th>\n      <th>first_cate_code</th>\n      <th>second_cate_code</th>\n      <th>item_code</th>\n      <th>ord_qty</th>\n    </tr>\n    <tr>\n      <th>order_date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-09-01</th>\n      <td>104</td>\n      <td>307</td>\n      <td>403</td>\n      <td>22069</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2015-09-01</th>\n      <td>104</td>\n      <td>301</td>\n      <td>405</td>\n      <td>20028</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2015-09-02</th>\n      <td>104</td>\n      <td>307</td>\n      <td>403</td>\n      <td>21183</td>\n      <td>109</td>\n    </tr>\n    <tr>\n      <th>2015-09-02</th>\n      <td>104</td>\n      <td>308</td>\n      <td>404</td>\n      <td>20448</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2015-09-02</th>\n      <td>104</td>\n      <td>307</td>\n      <td>403</td>\n      <td>21565</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2018-12-20</th>\n      <td>102</td>\n      <td>302</td>\n      <td>408</td>\n      <td>20994</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>2018-12-20</th>\n      <td>102</td>\n      <td>302</td>\n      <td>408</td>\n      <td>21875</td>\n      <td>502</td>\n    </tr>\n    <tr>\n      <th>2018-12-20</th>\n      <td>102</td>\n      <td>302</td>\n      <td>408</td>\n      <td>20215</td>\n      <td>106</td>\n    </tr>\n    <tr>\n      <th>2018-12-20</th>\n      <td>102</td>\n      <td>302</td>\n      <td>408</td>\n      <td>20195</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>2018-12-20</th>\n      <td>102</td>\n      <td>302</td>\n      <td>408</td>\n      <td>20321</td>\n      <td>205</td>\n    </tr>\n  </tbody>\n</table>\n<p>597694 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import gamma\n",
    "\n",
    "df = pd.read_csv(\"order_train1.csv\", encoding=\"gbk\")\n",
    "df[\"order_date\"]=df[\"order_date\"].apply(pd.to_datetime,format='%Y-%m-%d')\n",
    "# data.drop('order_date', axis=1, inplace=True)\n",
    "\n",
    "df = df.set_index('order_date')\n",
    "df_train = df[['sales_region_code', 'first_cate_code','second_cate_code','item_code','ord_qty']]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "     sales_region_code  item_code  first_cate_code  second_cate_code\n0                  101      20160              303               410\n1                  101      20175              304               409\n2                  101      20218              302               408\n3                  101      20240              306               407\n4                  101      20344              308               404\n..                 ...        ...              ...               ...\n99                 105      21420              307               403\n100                105      21875              302               408\n101                105      21892              303               410\n102                105      21895              308               404\n103                105      21964              303               410\n\n[104 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sales_region_code</th>\n      <th>item_code</th>\n      <th>first_cate_code</th>\n      <th>second_cate_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>20160</td>\n      <td>303</td>\n      <td>410</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>20175</td>\n      <td>304</td>\n      <td>409</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>20218</td>\n      <td>302</td>\n      <td>408</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101</td>\n      <td>20240</td>\n      <td>306</td>\n      <td>407</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>101</td>\n      <td>20344</td>\n      <td>308</td>\n      <td>404</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>105</td>\n      <td>21420</td>\n      <td>307</td>\n      <td>403</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>105</td>\n      <td>21875</td>\n      <td>302</td>\n      <td>408</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>105</td>\n      <td>21892</td>\n      <td>303</td>\n      <td>410</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>105</td>\n      <td>21895</td>\n      <td>308</td>\n      <td>404</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>105</td>\n      <td>21964</td>\n      <td>303</td>\n      <td>410</td>\n    </tr>\n  </tbody>\n</table>\n<p>104 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = pd.read_csv(\"pred-sales_region_code.csv\", encoding=\"gbk\")\n",
    "## 查看预测数据\n",
    "data_pre = pd.DataFrame(pre,columns=['sales_region_code', 'item_code','first_cate_code', 'second_cate_code',])\n",
    "data_pre\n",
    "# groupby(['sales_region_code'])['ord_qty'].sum().sort_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.36475408\n",
      "0.0\n",
      "0.037921347\n",
      "0.16826923\n",
      "0.0859375\n",
      "0.04363785\n",
      "0.04048583\n",
      "0.0\n",
      "0.0\n",
      "0.9062901\n",
      "0.0\n",
      "0.0\n",
      "0.04819277\n",
      "0.0\n",
      "0.0045506256\n",
      "0.18430033\n",
      "0.99999994\n",
      "0.0037140204\n",
      "0.260101\n",
      "0.14906833\n",
      "0.08945687\n",
      "0.0024813898\n",
      "0.0\n",
      "0.556196\n",
      "0.0\n",
      "0.29830325\n",
      "0.023529414\n",
      "0.07324841\n",
      "0.02247191\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.014466547\n",
      "0.020119224\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.1\n",
      "0.96103895\n",
      "0.0\n",
      "0.13978493\n",
      "0.0024813898\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0062160064\n",
      "0.0013088994\n",
      "0.2559899\n",
      "0.0\n",
      "0.19421488\n",
      "0.0\n",
      "0.013358778\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.037921347\n",
      "0.03068073\n",
      "0.25531915\n",
      "0.059405938\n",
      "0.0859375\n",
      "0.036546946\n",
      "1.0\n",
      "0.24390244\n",
      "1.0\n",
      "0.1\n",
      "0.0\n",
      "0.0884808\n",
      "0.04155844\n",
      "0.067307696\n",
      "0.0\n",
      "0.96103895\n",
      "1.0\n",
      "0.00057142857\n",
      "0.03125\n",
      "0.0\n",
      "0.0045506256\n",
      "0.260101\n",
      "0.0\n",
      "0.0\n",
      "0.003663004\n",
      "0.018610202\n",
      "0.556196\n",
      "0.0\n",
      "0.07746478\n",
      "0.036177106\n",
      "0.0\n",
      "0.0\n",
      "0.0066666678\n",
      "0.0\n",
      "0.64893615\n",
      "0.6048387\n",
      "0.3541667\n",
      "0.0\n",
      "0.99999994\n",
      "1.0\n",
      "0.32048193\n",
      "0.07746478\n",
      "0.06377551\n",
      "0.03125\n"
     ]
    }
   ],
   "source": [
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 指定 EarlyStopping 回调函数\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "predictions_df = pd.DataFrame(columns=['sales_region_code', 'item_code', 'first_cate_code', 'second_cate_code', ])\n",
    "\n",
    "# Loop over all sales_region_code and item_code combinations\n",
    "for i in range(len(data_pre)):\n",
    "    sales_region_code, item_code, first_cate_code, second_cate_code = data_pre.iloc[i, :]\n",
    "    # print(sales_region_code, item_code, first_cate_code, second_cate_code)\n",
    "    # print(sales_region_code, item_code, first_cate_code, second_cate_code)\n",
    "    filtered_df = df_train[(df_train['item_code'] == item_code) & (df_train['first_cate_code'] == first_cate_code) & (df_train['second_cate_code'] == second_cate_code)]\n",
    "\n",
    "    filtered_df_byday = filtered_df.groupby([pd.Grouper(freq='D')])['ord_qty'].mean().reset_index()\n",
    "    filtered_df_byday.set_index('order_date',inplace = True)\n",
    "    filtered_df_byday = filtered_df_byday.fillna(method='ffill')\n",
    "    # print(filtered_df)\n",
    "    values = filtered_df_byday['ord_qty'].values.reshape(-1,1)\n",
    "    # print(values.shape)\n",
    "    # print(values.shape[0])\n",
    "    # print(values.shape[1])\n",
    "\n",
    "    if values.shape[0]>0:\n",
    "\n",
    "        # values.to_numpy(values)\n",
    "        values = values.astype('float32')\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        train_size = int(len(scaled) * 0.8)\n",
    "        test_size = len(scaled) - train_size\n",
    "        train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\n",
    "        print(test[0][0])\n",
    "\n",
    "        if test_size > 1:\n",
    "\n",
    "                if test_size>30:\n",
    "                        look_back = 30\n",
    "                        print('>30', item_code, sales_region_code,first_cate_code, second_cate_code)\n",
    "                        X_train, y_train = create_dataset(train, look_back)\n",
    "                        X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "                if 30 >= test_size > 1:\n",
    "                        print('1-30', item_code, sales_region_code,first_cate_code, second_cate_code)\n",
    "                        look_back = test_size-1\n",
    "                        X_train, y_train = create_dataset(train, look_back)\n",
    "                        X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "\n",
    "                X_train, y_train = create_dataset(train, look_back)\n",
    "                X_test, y_test = create_dataset(test, look_back)\n",
    "                    # print(X_train.shape)\n",
    "                    # print(X_test.shape)\n",
    "\n",
    "                X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "                X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "                # Check if there is enough data to make a prediction\n",
    "                # 指定 EarlyStopping 回调函数\n",
    "                early_stop = EarlyStopping(monitor='val_loss', patience=20)\n",
    "                if len(X_test) > 0:\n",
    "                    # Create the RNN model\n",
    "                    model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                    tf.keras.layers.Dropout(0.2),\n",
    "                    tf.keras.layers.LSTM(32),\n",
    "                    tf.keras.layers.Dropout(0.2),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "                     # Compile the model\n",
    "                    model.compile(optimizer=Adam(0.0009), loss='mse')\n",
    "                     # Train the model\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        epochs=850,\n",
    "                        batch_size=32,\n",
    "                        validation_split=0.1,\n",
    "                        shuffle=False,\n",
    "                        callbacks=[early_stop]  # 添加 EarlyStopping 回调函数\n",
    "                    )\n",
    "                     # Evaluate the model\n",
    "                    # loss = model.evaluate(X_test, y_test)\n",
    "                    # print(loss)\n",
    "\n",
    "                    # Predict the test values using the trained model\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                    # 将 y_pred 和 y_test 逆归一化\n",
    "                    y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "                    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "                    # 计算 MSE\n",
    "                    mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "                    # print(\"MSE: \", mse)\n",
    "                    tot_mse = tf.reduce_sum(mse)\n",
    "                    avg_mse = tot_mse / mse.shape[0]\n",
    "                    print('Test MSE: %.3f' % avg_mse)\n",
    "\n",
    "                    # 设置x轴标签的格式\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                    plt.plot(filtered_df_byday.index[-len(y_test):], y_test, label='Actual')\n",
    "                    plt.plot(filtered_df_byday.index[-len(y_pred):], y_pred, label='Predicted')\n",
    "                    plt.title('sales_region_code_' + str(int(sales_region_code)) + ' & ' + 'item_code_' + str(int(item_code))+ '\\nfirst_cate_code_' + str(int(first_cate_code)) + ' & ' + 'second_cate_code_' + str(int(second_cate_code)))\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                    # 预测未来days_to_predict天的订单数量\n",
    "                    days_to_predict_1_month = 30\n",
    "                    days_to_predict_2_month = 60\n",
    "                    days_to_predict_3_month = 90\n",
    "\n",
    "                    # 添加这个函数用于预测未来的订单数量\n",
    "                    def predict_future(model, x_input, days_to_predict):\n",
    "                        future_predictions = []\n",
    "                        for _ in range(days_to_predict):\n",
    "                            # 预测未来一天的订单数量\n",
    "                            predicted_value = model.predict(x_input[np.newaxis, ...])\n",
    "                            # 将预测值添加到future_predictions列表中\n",
    "                            future_predictions.append(predicted_value[0][0])\n",
    "                            # 将x_input中的第一列（订单数量）替换为预测值\n",
    "                            x_input[:, 0] = np.append(x_input[1:, 0], predicted_value)\n",
    "                        return future_predictions\n",
    "\n",
    "                    # 提取最后一个时间窗口的输入数据\n",
    "                    x_input = X_test[-1]\n",
    "                    # print(X_test[-1])\n",
    "                    # print(scaler.inverse_transform(np.array(X_test[-1]).reshape(-1, 1)).flatten())\n",
    "\n",
    "                    # 使用模型进行未来30天的预测\n",
    "                    future_predictions_1_month = predict_future(model, x_input, days_to_predict_1_month)\n",
    "                    print(future_predictions_1_month)\n",
    "                    future_predictions_2_month = predict_future(model, x_input, days_to_predict_2_month)\n",
    "                    future_predictions_3_month = predict_future(model, x_input, days_to_predict_3_month)\n",
    "\n",
    "\n",
    "                    # 将预测结果逆归一化\n",
    "                    future_sum_1_month = sum(scaler.inverse_transform(np.array(future_predictions_1_month).reshape(-1, 1)).flatten())\n",
    "                    future_sum_2_month = sum(scaler.inverse_transform(np.array(future_predictions_2_month).reshape(-1, 1)).flatten()) - future_sum_1_month\n",
    "                    future_sum_3_month = sum(scaler.inverse_transform(np.array(future_predictions_3_month).reshape(-1, 1)).flatten()) - future_sum_2_month - future_sum_1_month\n",
    "\n",
    "\n",
    "                    # 将销售区域代码、物品代码、一级类别代码、二级类别代码以及未来30天的预测值总和追加到predictions_df\n",
    "                    predictions_df = predictions_df.append({\n",
    "                        'sales_region_code': sales_region_code,\n",
    "                        'item_code': item_code,\n",
    "                        'first_cate_code': first_cate_code,\n",
    "                        'second_cate_code': second_cate_code,\n",
    "                        'mse': avg_mse,\n",
    "                        'prediction_1_month': future_sum_1_month,\n",
    "                        'prediction_2_month': future_sum_2_month,\n",
    "                        'prediction_3_month': future_sum_3_month\n",
    "                    }, ignore_index=True)\n",
    "        if test_size == 1:\n",
    "                # 将销售区域代码、物品代码、一级类别代码、二级类别代码以及未来30天的预测值总和追加到predictions_df\n",
    "                predictions_df = predictions_df.append({\n",
    "                    'sales_region_code': sales_region_code,\n",
    "                    'item_code': item_code,\n",
    "                    'first_cate_code': first_cate_code,\n",
    "                    'second_cate_code': second_cate_code,\n",
    "                    'mse': 0,\n",
    "                    'prediction_1_month': test[0][0],\n",
    "                    'prediction_2_month': test[0][0],\n",
    "                    'prediction_3_month': test[0][0]\n",
    "                }, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "                predictions_df = predictions_df.append({\n",
    "                    'sales_region_code': sales_region_code,\n",
    "                    'item_code': item_code,\n",
    "                    'first_cate_code': first_cate_code,\n",
    "                    'second_cate_code': second_cate_code,\n",
    "                    'mse': 0,\n",
    "                    'prediction_1_month': 0,\n",
    "                    'prediction_2_month': 0,\n",
    "                    'prediction_3_month': 0\n",
    "                }, ignore_index=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# predictions_df.to_csv(\"predictions-无-sales_region_code.csv\", index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
